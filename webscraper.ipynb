{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7dc7a1-42dc-43fa-97e4-cd946e96a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "# this function helps me retriev the webpages \n",
    "def extract_content(url):\n",
    "    executable_path=r\"C:\\Users\\PC\\Desktop\\projects\\Web_Scraper\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "    options = Options()\n",
    "    service=Service(executable_path=executable_path)\n",
    "    options.add_argument('--headless')  \n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    # You can specify the path to chromedriver if it's not in PATH\n",
    "    driver = webdriver.Chrome(service=service,options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Wait for full page to load (can adjust)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    # Remove unwanted elements\n",
    "    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator='\\n', strip=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f0da8d-f2ce-4322-b3dc-6a04ba392803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "#this function helps me fine the urls which mustly correspond to the question i ask\n",
    "def search_url(question,results=3):\n",
    "    urls=[]\n",
    "    with DDGS() as ddgs:\n",
    "        for r in ddgs.text(question,max_results=results):\n",
    "            urls.append(r['href'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55817fd4-5456-46c6-9609-fb7ad36f9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#web search for urls\n",
    "question=input()\n",
    "urls=search_url(question)\n",
    "for i,url in enumerate(urls,0):\n",
    "    print(f\"{i},{url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a554408-1fe9-4004-84ee-f630c7393b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this prompt i try to summarize the content of the pages a retrieve using an llm\n",
    "def summary_prompt(url_content):\n",
    "    prompt = f\"\"\"You are a helpful assistant tasked with summarizing a web article.\n",
    "\n",
    "Your goal is to provide a clear, concise, and accurate summary that captures all key points and nuances from the original text. Focus only on the relevant information that answers the user's query context.\n",
    "\n",
    "Guidelines:\n",
    "- Be faithful to the original meaning.\n",
    "- Do not omit important details.\n",
    "- Use your own words avoid copying long phrases.\n",
    "- Keep the summary under approximately 1500 words.\n",
    "- Ensure the summary is well-organized and easy to read.\n",
    "-Provide plain text only.\n",
    "-no numbering.\n",
    "\n",
    "\n",
    "Text to summarize:\n",
    "{url_content}\n",
    "\n",
    "Respond only with the final summary text below:\n",
    "summary\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50861577-ffb8-4264-95f1-78db1b829087",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"mistral\"\n",
    "#this function is responsible for the request and Response\n",
    "def Query_llm(prompt: str, model: str, host: str = \"http://localhost:11434\"):\n",
    "    url = f\"{host}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False  \n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.text.strip()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Ollama API error:\", e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fffe5a9-123d-4488-9f78-e3323d502f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state={}\n",
    "state[\"question\"]=question\n",
    "state[\"context\"]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb4402-8f8b-484e-a0cd-0cf09763f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i ,link in enumerate(urls):\n",
    "    print(state[\"question\"])\n",
    "    state[\"context\"][f\"link{i}\"]=link\n",
    "    print(state[\"context\"][f\"link{i}\"])\n",
    "    url_content=extract_content(link)\n",
    "    prompt=summary_prompt(url_content)\n",
    "    summary=Query_llm(prompt,model)\n",
    "    state[\"context\"][f\"summary{i}\"]=summary\n",
    "    print(state[\"context\"][f\"summary{i}\"][\"response\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60203e32-2660-4f86-b758-bff1539a55e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
